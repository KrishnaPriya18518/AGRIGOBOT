{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoaRo8prKJ07"
      },
      "source": [
        " pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_h8pz4tLzE8"
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5IYwlK0L-Ny"
      },
      "source": [
        "#import libraries\n",
        "from newspaper import Article\n",
        "\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from IPython import display\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "warnings.filterwarnings('ignore')\n",
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('intents.json').read()\n",
        "intents = json.loads(data_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268x02-S6sGq"
      },
      "source": [
        "nltk.download('punkt',quiet=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhATlt24p7ke"
      },
      "source": [
        "#pre processing data\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        #add documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-SxgYfSqCEC"
      },
      "source": [
        "# lemmatize, lower each word and remove duplicates\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fytEkxYmq3dd"
      },
      "source": [
        "# create our training data\n",
        "training = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # lemmatize each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "# create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktOElYC9v-Q1"
      },
      "source": [
        "example1 = \"/content/agricitease1.txt\"\n",
        "file1 = open(example1, \"r\")\n",
        "FileContent = file1.read()\n",
        "\n",
        "print(FileContent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdNhZjCUxsvj"
      },
      "source": [
        "text = FileContent\n",
        "agricitease_list = nltk.sent_tokenize(text)\n",
        "print(agricitease_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-rYcWQnnPoT"
      },
      "source": [
        "agricitease_list1 = ['agricitease','farmer','consumer','representative','delivery timmigs','delivery','features','farmer features','consumer features','representative features','web application','website']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekgsRC1pmheI"
      },
      "source": [
        "!pip install fontstyle --upgrade\n",
        "import fontstyle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlC-L8w2M5dr"
      },
      "source": [
        "nltk.download('punkt',quiet=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKHOuHdmNAaG"
      },
      "source": [
        "article = Article('https://agritech.tnau.ac.in/agricultural_marketing/agrimark_India.html')\n",
        "article.download()\n",
        "article.parse()\n",
        "article.nlp()\n",
        "agri = article.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USMq11K_NhPa"
      },
      "source": [
        "print(agri)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMGt3dc7Patt"
      },
      "source": [
        "#tokenization\n",
        "text = agri\n",
        "sentence_list = nltk.sent_tokenize(text) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eey1SaZQUAbG"
      },
      "source": [
        "print(sentence_list)\n",
        "print(agricitease_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NazVlplRUFBo"
      },
      "source": [
        "# function to return random greeting response\n",
        "def greeting_response(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  #bots greetinng response\n",
        "  bot_greetings = ['Hi there, how can we help you today?','Hi, Good to see you, do you need something?','Hey! Can I help you with something?','Hey! I am glad that you are here. Please tell me, how can I help you?']\n",
        "  #users greetings\n",
        "  user_greetings= ['hi','hellooo','hello','greetings','wassup']\n",
        "\n",
        "  for word in text.split():\n",
        "    if word in user_greetings:\n",
        "      return random.choice(bot_greetings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGvijks7BHVg"
      },
      "source": [
        "# function to return random goodbye response\n",
        "def goodbye_response(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  #bots goodbye response\n",
        "  bot_goodbye = ['Bye!Have a good day!','Goodbye','Come visit us again.Bye-Bye!','See you later!']\n",
        "  #users goodbye\n",
        "  user_goodbye= ['byee','goodbye','adios','bye']\n",
        "\n",
        "  for word in text.split():\n",
        "    if word in user_goodbye:\n",
        "      return random.choice(bot_goodbye)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJWz_AducSI1"
      },
      "source": [
        "#funtion to return about agrigobot\n",
        "def who_response(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  #bots response\n",
        "  bot_whoresponse = ['Iam your very own AgriGo bot, here to help!',\"I'm AgriGo bot, here to assist you!\"]\n",
        "  #users ques\n",
        "  user_ques =['who are you?','what is your name?', 'what do you do?']\n",
        "\n",
        "\n",
        "  if text in user_ques:\n",
        "    return random.choice(bot_whoresponse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTh_GNRZ3v3i"
      },
      "source": [
        "# function to return random thank-you response\n",
        "def thanks_response(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  #bots thanks response\n",
        "  bot_thanks = ['You got it','Don’t mention it','No worries','Not a problem','My pleasure','It was nothing','I’m happy to help','Anytime']\n",
        "  #users thanks\n",
        "  user_thanks= ['Thanks so much','Thanks a lot','Thanks a bunch','Thanks a ton', 'Thanks','thanks','thank you','thankyou']\n",
        "\n",
        "  for word in text.split():\n",
        "    if word in user_thanks:\n",
        "      return random.choice(bot_thanks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK6h_DwJAwfc"
      },
      "source": [
        "# function to return for unmatched statements\n",
        "def idk_response(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  #bots idk response\n",
        "\n",
        "  bot_idk = [\"What did you say?\",\"I don't understand.\",\"Excuse me, I didn't get it.\",\"Excuse me, can you please repeat it?\",\"Sorry, I did not catch that.\",\"I missed that.\",\"I don't get it.\",\"I’m afraid it is not clear what you saying.\",\"I am sorry, but I don’t follow what you are saying.\",\"I don’t catch what you said.Sorry.\"]\n",
        "\n",
        "  return random.choice(bot_idk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXu9jlADKgkM"
      },
      "source": [
        "def agricitease_response(user_input):\n",
        "\n",
        "  user_input = user_input.lower()\n",
        "  user_input1= user_input.split(\" \")\n",
        "  for word in user_input1:\n",
        "    if word in agricitease_list1:\n",
        "      agricitease_list.append(word)\n",
        "      agricitease_response = ''\n",
        "      cm = CountVectorizer().fit_transform(agricitease_list)\n",
        "      similarity_scores = cosine_similarity(cm[-1],cm)\n",
        "      similarity_scores_list = similarity_scores.flatten()\n",
        "      index = index_sort(similarity_scores_list)\n",
        "      index = index[1:]\n",
        "      response_flag = 0\n",
        "      j = 0\n",
        "      for i in range(len(index)):\n",
        "        if similarity_scores_list[index[i]] > 0.0:\n",
        "          colour = '\\33[34m'\n",
        "          agricitease_response = (fontstyle.apply('\\33[34m' \"Agri bot: \"  + agricitease_list[index[i]],'bold/Italic'))\n",
        "          response_flag = 1\n",
        "          j = j+1\n",
        "          if j >2:\n",
        "            break\n",
        "          if response_flag == 0:\n",
        "            color = '\\33[32m'\n",
        "            agricitease_response = (fontstyle.apply('\\33[32m' \"Agri bot: \" +idk_response(user_input),'bold/Italic'))\n",
        "            agricitease_list.remove(user_input)\n",
        "      return agricitease_response\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRRXL_N6Vybd"
      },
      "source": [
        "def index_sort(list_var):\n",
        "  length = len(list_var)\n",
        "  list_index = list(range(0, length))\n",
        "\n",
        "  x = list_var\n",
        "  for i in range(length):\n",
        "    for j in range(length):\n",
        "      if x[list_index[i]] > x[list_index[j]]:\n",
        "        #swap\n",
        "        temp = list_index[i]\n",
        "        list_index[i] = list_index[j]\n",
        "        list_index[j] = temp\n",
        "\n",
        "  return list_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz91JAM0Uh7R"
      },
      "source": [
        "# create bot response\n",
        "def bot_response(user_input):\n",
        "  user_input = user_input.lower()\n",
        "  sentence_list.append(user_input)\n",
        "  bot_response = ''\n",
        "  cm = CountVectorizer().fit_transform(sentence_list)\n",
        "  similarity_scores = cosine_similarity(cm[-1],cm)\n",
        "  similarity_scores_list = similarity_scores.flatten()\n",
        "  index = index_sort(similarity_scores_list)\n",
        "  index = index[1:]\n",
        "  response_flag = 0\n",
        "\n",
        "  j = 0\n",
        "  for i in range(len(index)):\n",
        "    if similarity_scores_list[index[i]] > 0.0:\n",
        "      colour = '\\33[34m'\n",
        "      bot_response = (fontstyle.apply('\\33[34m' \"Agri bot: \"  + sentence_list[index[i]],'bold/Italic'))\n",
        "      response_flag = 1\n",
        "      j = j+1\n",
        "    if j >2:\n",
        "      break\n",
        "\n",
        "  if response_flag == 0:\n",
        "    color = '\\33[32m'\n",
        "    bot_response = (fontstyle.apply('\\33[32m' \"Agri bot: \" + idk_response(user_input),'bold/Italic'))\n",
        "\n",
        "  sentence_list.remove(user_input)\n",
        "   \n",
        "  return bot_response\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkKujK9aoFNA"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "#fitting and saving the model \n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "print(\"model created\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdR666n0rrcS"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "model = load_model('chatbot_model.h5')\n",
        "import json\n",
        "import random\n",
        "intents = json.loads(open('intents.json').read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBTSXRpM9UT0"
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern - split words into array\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word - create short form for word\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0]*len(words) \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQiK7_rrBshC"
      },
      "source": [
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "def chatbot_response(text):\n",
        "    ints = predict_class(text, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYS4EJbnxPQG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YazYV26JY2DR",
        "outputId": "433fbbb6-9447-4a08-f567-d2b770021c1f"
      },
      "source": [
        "#start chat\n",
        "Red_Font = '\\33[31m'\n",
        "print(Red_Font + fontstyle.apply(\"\\033[3mAgri Bot: I am Agri bot. I will answer your queries. If u want to exit please type: bye\",'bold/Italic'))\n",
        "\n",
        "\n",
        "while(True):\n",
        "  user_input = input()\n",
        "  if goodbye_response(user_input) != None  :\n",
        "    print(fontstyle.apply('\\33[33m' \"Agri bot: \"+goodbye_response(user_input),'bold/Italic'))\n",
        "    break\n",
        "  else:\n",
        "    if greeting_response(user_input) != None :\n",
        "\n",
        "      print(fontstyle.apply('\\33[36m' \"Agri bot: \"+ greeting_response(user_input),'bold/Italic'))\n",
        "    elif agricitease_response(user_input) != None :\n",
        "      print(fontstyle.apply( agricitease_response(user_input),'bold/Italic'))\n",
        "\n",
        "\n",
        "    elif who_response(user_input) != None:\n",
        "      print(fontstyle.apply('\\33[36m' \"Agri bot: \"+ who_response(user_input),'bold/Italic'))\n",
        "    elif thanks_response(user_input) != None:\n",
        "      print(fontstyle.apply('\\33[36m' \"Agri bot: \"+ thanks_response(user_input),'bold/Italic'))\n",
        "   \n",
        "    elif:\n",
        "      \n",
        "      print(fontstyle.apply('\\33[36m' \"Agri bot: \"+ chatbot_response(user_input),'bold/Italic'))\n",
        "    else:\n",
        "      print(fontstyle.apply('\\33[36m' \"Agri bot: \"+ bot_response(user_input),'bold/Italic'))\n",
        "display.Image(\"logo.jpg\",width=300,height=300)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1m\u001b[3m\u001b[3mAgri Bot: I am Agri bot. I will answer your queries. If u want to exit please type: bye\u001b[0m\n",
            "who are you?\n",
            "\u001b[1m\u001b[3m\u001b[36mAgri bot: I'm AgriGo bot, here to assist you!\u001b[0m\n",
            "what is agricitease\n",
            "\u001b[1m\u001b[3m\u001b[1m\u001b[3m\u001b[34mAgri bot:  \"AGRICITEASE\" affinity founding between Farmers and Consumers\n",
            " is an all-inclusive digital platform designed to assist farmers in selling their products The new move will eliminate the need for farmers to sell their products through retailers that will escalate in farmer's profits.\u001b[0m\u001b[0m\n",
            "what is your name?\n",
            "\u001b[1m\u001b[3m\u001b[36mAgri bot: I'm AgriGo bot, here to assist you!\u001b[0m\n",
            "are you an idiot?\n",
            "\u001b[94m\u001b[7m\u001b[1m\u001b[4m\u001b[36mAgri bot: Hi there, how can I help?\u001b[0m bold/Italic\n",
            "are u mad?\n",
            "\u001b[94m\u001b[7m\u001b[1m\u001b[4m\u001b[36mAgri bot: Hi there, how can I help?\u001b[0m bold/Italic\n",
            "u r waste\n",
            "\u001b[94m\u001b[7m\u001b[1m\u001b[4m\u001b[36mAgri bot: Good to see you again\u001b[0m bold/Italic\n",
            "where do u live\n",
            "\u001b[94m\u001b[7m\u001b[1m\u001b[4m\u001b[36mAgri bot: Hello, thanks for asking\u001b[0m bold/Italic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR7W21UA5MZM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}